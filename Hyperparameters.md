# Hyperparameter tuning

We have considered the following values for hyperparameters (non-exhaustive) to determine settings of models applied in our study:


### Number of epochs
130, 200, 300, 400, 500, 600, 650, 780, 800, 814, 823, 910, 931, 1000, 1027, 1040, 1046, 1055, 1083, 1145, 1166, 1170, 1200, 1208, 1214, 1249, 1250, 1267, 1300, 1316, 1324, 1325, 1400, 1412, 1430, 1432, 1458, 1468, 1500, 1501, 1512, 1542, 1551, 1560, 1567, 1585, 1600, 1690, 1721, 1795, 1796, 1798, 1800, 1820, 1825, 1846, 1878, 1891, 1950, 1954, 2000, 2200, 2500, 2600, 3200


### Latent dimensions
7, 8, 9, 10, 11, 12, 13, 14, 15, 17, 18, 19, 20, 21, 23, 24, 25, 26, 29, 30, 31, 32, 33, 34, 37

### VAE layers
'(100, 100, 30)', '(100, 75, 50)', '(128, 32)', '(150, 100)', '(45, 25, 25, 25)', '(45, 30, 30, 30)', '(50, 30, 30)', '(50, 45, 35, 30)', '(60, 50, 40, 30)', '(75, 60, 30)', '(75, 60, 30, 15)', '(75, 60, 30, 20)', '(75, 60, 30, 25)', '(75, 60, 30, 30)'

### Learning rate
0.00308, 0.00364, 0.0042, 0.00476, 0.006, 0.00826, 0.0084, 0.00896, 0.00966, 0.01, 0.01008, 0.01078, 0.011, 0.01162, 0.01274, 0.01288, 0.01302, 0.01386, 0.014, 0.015, 0.01568, 0.01596, 0.01694, 0.01778, 0.01792, 0.01904, 0.01974, 0.02058, 0.02128, 0.02156, 0.0238, 0.02436, 0.02534, 0.02688, 0.02758, 0.02982, 0.03052, 0.03402, 0.03472
